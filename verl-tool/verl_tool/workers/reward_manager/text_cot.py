# Copyright 2024 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import torch
import json
import time
import numpy as np
import regex as re
from typing import Dict, List, Any, Tuple, Optional
from collections import defaultdict
from pathlib import Path
from verl import DataProto
from verl.workers.reward_manager import register
#from .utils import replace_consecutive_tokens
from verl_tool.workers.reward_manager.utils import replace_consecutive_tokens

# ============= ğŸ”§ æ ¸å¿ƒå¥–åŠ±å‡½æ•° =============

def parse_ranking_from_response(response_str: str) -> Tuple[Optional[List[int]], bool]:
    """
    ä»å“åº”ä¸­è§£ææ’åºåˆ—è¡¨
    æ”¯æŒæ ¼å¼ï¼š[2,3,4,1,5] æˆ– [2, 3, 4, 1, 5]
    """
    is_list_format = False
    pattern = re.compile(r"<answer>(.*?)</answer>", re.DOTALL)
    match = pattern.search(response_str)
    if not match:
        return None, is_list_format
    
    answer_content = match.group(1).strip()
    
    
    # å°è¯•è§£æä¸ºåˆ—è¡¨
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ—è¡¨æ ¼å¼
        if answer_content.startswith('[') and answer_content.endswith(']'):
            ranking = eval(answer_content)
            if isinstance(ranking, list) and all(isinstance(x, int) for x in ranking):
                is_list_format = True
                return ranking, is_list_format
        
        # å®½æ¾æ¨¡å¼ï¼šæå–æ‰€æœ‰æ•°å­—
        numbers = re.findall(r'\d+', answer_content)
        if numbers:
            ranking = [int(x) for x in numbers]
            is_list_format = False
            return ranking, is_list_format
    except:
        pass
    
    return None, is_list_format


def compute_format_reward(response_str: str, num_candidates: int) -> Dict[str, float]:
    """
    è®¡ç®—æ ¼å¼å¥–åŠ±
    
    æ­¥éª¤ï¼š
    1. æ£€æŸ¥ <think>...</think><answer>...</answer> æ ¼å¼ï¼ˆfullmatchï¼‰
    2. å¦‚æœ fullmatchï¼Œformat_reward = 1.0
    3. åœ¨ 1.0 åŸºç¡€ä¸Šæ·»åŠ æƒ©ç½šï¼š
       - ç´¢å¼•è¶Šç•Œæƒ©ç½š
       - é•¿åº¦ä¸ä¸€è‡´æƒ©ç½š
       - ä¸æ˜¯ [x,x,x] æ ¼å¼çš„æƒ©ç½š
    
    Returns:
        åŒ…å«å„é¡¹å¾—åˆ†çš„å­—å…¸
    """
    rewards = {
        'format_reward': 0.0,
        'is_valid_format': 0.0,
        'is_list_format': 0.0,
        'index_penalty': 0.0,
        'length_penalty': 0.0,
    }
    
    # æ£€æŸ¥ <think>...</think><answer>...</answer> æ ¼å¼ï¼ˆä½¿ç”¨ fullmatchï¼‰
    pattern = re.compile(r"<think>(.*?)</think>.*<answer>(.*?)</answer>.*", re.DOTALL)
    format_match = re.fullmatch(pattern, response_str)
    
    if not format_match:
        # ä¸ç¬¦åˆåŸºç¡€æ ¼å¼ï¼Œformat_reward = 0
        return rewards
    
    # ç¬¦åˆåŸºç¡€æ ¼å¼ï¼Œformat_reward ä» 1.0 å¼€å§‹
    rewards['is_valid_format'] = 1.0
    rewards['format_reward'] = 1.0
    
    # å°è¯•è§£ææ’åºåˆ—è¡¨
    ranking, is_list_format = parse_ranking_from_response(response_str)
    
    if ranking is None or not is_list_format:
        # ä¸æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œæ‰£é™¤æƒ©ç½š
        rewards['format_reward'] -= 0.8  # ä¸æ˜¯åˆ—è¡¨æ ¼å¼æƒ©ç½š
        return rewards
    
    # æ˜¯åˆ—è¡¨æ ¼å¼
    rewards['is_list_format'] = 1.0
    
    # æ£€æŸ¥ç´¢å¼•è¶Šç•Œ
    invalid_indices = [idx for idx in ranking if idx < 1 or idx > num_candidates]
    if invalid_indices:
        # ç´¢å¼•è¶Šç•Œæƒ©ç½šï¼šæ¯ä¸ªè¶Šç•Œç´¢å¼•æ‰£ 0.2/num_candidates
        index_penalty = -0.8
        rewards['index_penalty'] = index_penalty
        rewards['format_reward'] += index_penalty
    
    # æ£€æŸ¥é•¿åº¦ä¸ä¸€è‡´
    if len(ranking) != num_candidates:
        # é•¿åº¦ä¸ä¸€è‡´æƒ©ç½šï¼šå·®å¼‚è¶Šå¤§æƒ©ç½šè¶Šé‡
        length_diff = abs(len(ranking) - num_candidates)
        length_penalty = -0.8
        rewards['length_penalty'] = length_penalty
        rewards['format_reward'] += length_penalty
    
    # ç¡®ä¿ format_reward åœ¨ [0, 1] èŒƒå›´å†…
    rewards['format_reward'] = max(0.0, min(1.0, rewards['format_reward']))
    
    return rewards


def compute_ranking_reward(
    predicted_ranking: List[int], 
    ground_truth_position: int,
    sigma: float = 0.5
) -> Tuple[float, Dict[str, float]]:
    """
    ä½¿ç”¨é«˜æ–¯æ ¸è®¡ç®—æ’åºå¥–åŠ±
    
    åªä½¿ç”¨é«˜æ–¯æ ¸ï¼Œsigma = 0.5
    
    Args:
        predicted_ranking: é¢„æµ‹æ’åº [2,3,4,1,5]
        ground_truth_position: çœŸå®ç­”æ¡ˆä½ç½®
        sigma: é«˜æ–¯æ ¸å®½åº¦ï¼Œé»˜è®¤ 0.5
    
    Returns:
        (gaussian_reward, detailed_scores)
    """
    if not predicted_ranking:
        return 0.0, {'rank_position': -1, 'gaussian_reward': 0.0}
    
    # æ‰¾åˆ°çœŸå®ç­”æ¡ˆåœ¨é¢„æµ‹æ’åºä¸­çš„ä½ç½®
    try:
        rank_of_gt = predicted_ranking.index(ground_truth_position) + 1  # 1-indexed
    except ValueError:
        # çœŸå®ç­”æ¡ˆä¸åœ¨æ’åºä¸­
        return 0.0, {'rank_position': -1, 'gaussian_reward': 0.0}
    
    # é«˜æ–¯æ ¸å¥–åŠ±: exp(-((rank - 1)^2) / (2 * sigma^2))
    # rank=1 â†’ reward=1.0
    # rank=2 â†’ reward=0.32 (sigma=0.5)
    # rank=3 â†’ reward=0.02 (sigma=0.5)
    gaussian_reward = np.exp(-((rank_of_gt - 1) ** 2) / (2 * sigma ** 2))
    
    detailed_scores = {
        'rank_position': rank_of_gt,
        'gaussian_reward': float(gaussian_reward),
    }
    
    return gaussian_reward, detailed_scores


def compute_reward(
    response_str: str,
    ground_truth_position: int,
    num_candidates: int,
    sigma: float = 0.5,
    format_weight: float = 0.3,
    ranking_weight: float = 0.7
) -> Dict[str, float]:
    """
    ç»¼åˆå¥–åŠ±è®¡ç®—
    
    æœ€ç»ˆå¥–åŠ± = format_weight * format_reward + ranking_weight * ranking_reward
    æ€»æƒé‡å’Œä¸º1ï¼ˆformat_weight + ranking_weight = 1ï¼‰
    
    Args:
        response_str: æ¨¡å‹å“åº”
        ground_truth_position: çœŸå®ç­”æ¡ˆä½ç½®
        num_candidates: å€™é€‰æ•°é‡
        sigma: é«˜æ–¯æ ¸å®½åº¦
        format_weight: æ ¼å¼å¥–åŠ±æƒé‡
        ranking_weight: æ’åºå¥–åŠ±æƒé‡
    
    Returns:
        åŒ…å«æ‰€æœ‰å¾—åˆ†çš„å­—å…¸
    """
    # 1. è®¡ç®—æ ¼å¼å¥–åŠ±
    format_scores = compute_format_reward(response_str, num_candidates)
    format_reward = format_scores['format_reward']
    
    # 2. è§£ææ’åº
    ranking, is_list_format = parse_ranking_from_response(response_str)
    
    if ranking is None:
        # æ— æ³•è§£ææ’åºï¼Œåªè¿”å›æ ¼å¼åˆ†
        return {
            **format_scores,
            'ranking_reward': 0.0,
            'rank_position': -1,
            'gaussian_reward': 0.0,
            'final_reward': format_weight * format_reward,
        }
    
    # 3. è®¡ç®—æ’åºå¥–åŠ±
    ranking_reward, ranking_details = compute_ranking_reward(
        ranking, ground_truth_position, sigma=sigma
    )
    
    # 4. ç»„åˆæœ€ç»ˆå¥–åŠ±
    final_reward = format_weight * format_reward + ranking_weight * ranking_reward
    
    return {
        **format_scores,
        **ranking_details,
        'ranking_reward': ranking_reward,
        'final_reward': final_reward,
    }


# ============= ğŸ”§ å¥–åŠ±ç®¡ç†å™¨ =============

@register("text_cot")
class TextCoTRewardManager:
    """
    çº¯æ–‡æœ¬CoTæ’åºä»»åŠ¡å¥–åŠ±ç®¡ç†å™¨
    """
    name = "text_cot"
    
    def __init__(self, tokenizer, num_examine, compute_score=None, reward_fn_key='data_source', **kwargs) -> None:
        self.tokenizer = tokenizer
        self.num_examine = num_examine
        self.reward_fn_key = reward_fn_key
        self.step = None
        
        # æ’åºå¥–åŠ±å‚æ•°
        self.gaussian_sigma = kwargs.get('gaussian_sigma', 0.5)  # é«˜æ–¯æ ¸å®½åº¦
        self.format_weight = kwargs.get('format_weight', 0.3)    # æ ¼å¼å¥–åŠ±æƒé‡
        self.ranking_weight = kwargs.get('ranking_weight', 0.7)  # æ’åºå¥–åŠ±æƒé‡
        
        if "record_dir" in kwargs:
            self.record_dir = Path(kwargs['record_dir'])
            self.record_dir.mkdir(parents=True, exist_ok=True)
    
    def __call__(self, data: DataProto, return_dict=False):
        """è®¡ç®—å¥–åŠ±"""
        save_record = data.meta_info.get('save_record', True)

        # åˆå§‹åŒ–è®°å½•ç›®å½•
        if not hasattr(self, 'record_dir'):
            if hasattr(self, 'run_id'):
                self.record_dir = Path(__file__).parent.parent.parent.parent / "verl_step_records" / self.run_id
            else:
                import time
                self.record_dir = Path(__file__).parent.parent.parent.parent / "verl_step_records" / f"text_cot-{time.strftime('%Y%m%d-%H%M%S')}"
                self.record_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥stepç´¢å¼•
        if self.step is None:
            last_step_idx = 0
            import os
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            if not self.record_dir.exists():
                self.record_dir.mkdir(parents=True, exist_ok=True)
            for file in os.listdir(self.record_dir):
                if self.num_examine == 1:
                    if re.search(r"step-val-\d+\.json", file):
                        step_idx = int(file[:-len(".json")].split("-")[-1])
                        if step_idx > last_step_idx:
                            last_step_idx = step_idx
                else:
                    if re.search(r"step-\d+\.json", file):
                        step_idx = int(file[:-len(".json")].split("-")[-1])
                        if step_idx > last_step_idx:
                            last_step_idx = step_idx
            self.step = last_step_idx + 1
        
        if data.meta_info.get('global_step', None) is not None:
            self.step = data.meta_info['global_step']

        # å¦‚æœå·²æœ‰rm_scoresï¼Œç›´æ¥è¿”å›
        if 'rm_scores' in data.batch.keys():
            if return_dict:
                return {"reward_tensor": data.batch['rm_scores']}
            else:
                return data.batch['rm_scores']

        reward_tensor = torch.zeros_like(data.batch['responses'], dtype=torch.float32)
        from collections import defaultdict
        reward_extra_info = defaultdict(list)

        already_print_data_sources = {}
        to_save_records = []

        for i in range(len(data)):
            data_item = data[i]

            # è§£ç promptå’Œresponse
            prompt_ids = data_item.batch['prompts']
            prompt_length = prompt_ids.shape[-1]
            valid_prompt_length = data_item.batch['attention_mask'][:prompt_length].sum()
            valid_prompt_ids = prompt_ids[-valid_prompt_length:]

            response_ids = data_item.batch['responses']
            valid_response_length = data_item.batch['attention_mask'][prompt_length:].sum()
            valid_response_ids = response_ids[:valid_response_length]
            if "loss_mask" in data_item.batch:
                loss_mask = data_item.batch['loss_mask']
                valid_response_ids_with_loss_mask = torch.where(loss_mask[prompt_length:prompt_length + valid_response_length] == 1, valid_response_ids, self.tokenizer.pad_token_id)
            else:
                valid_response_ids_with_loss_mask = valid_response_ids

            # è§£ç 
            prompt_str = self.tokenizer.decode(valid_prompt_ids, skip_special_tokens=True)
            response_str = self.tokenizer.decode(valid_response_ids, skip_special_tokens=True)

            # è·å–é¢å¤–ä¿¡æ¯
            extra_info = data_item.non_tensor_batch.get('extra_info', {})
            ground_truth_position = extra_info.get('ground_truth_position', 1)
            num_candidates = extra_info.get('num_candidates', 5)
            data_source = data_item.non_tensor_batch.get(self.reward_fn_key, 'unknown')
            
            # ============= è®¡ç®—å¥–åŠ± =============
            
            # åŸºç¡€å¥–åŠ±ï¼ˆæ ¼å¼ + æ’åºï¼‰
            base_scores = compute_reward(
                response_str,
                ground_truth_position,
                num_candidates,
                sigma=self.gaussian_sigma,
                format_weight=self.format_weight,
                ranking_weight=self.ranking_weight
            )
            
            final_reward = base_scores['final_reward']
            
            # é™åˆ¶åœ¨åˆç†èŒƒå›´
            final_reward = max(min(final_reward, 1.5), -0.5)
            
            # åˆå¹¶æ‰€æœ‰å¾—åˆ†
            all_scores = {
                **base_scores,
                'final_reward': final_reward,
            }

            all_scores["accuracy"] = 1 if final_reward > 0.6 else 0
            if all_scores['accuracy'] > 0:
                reward_extra_info['correct_response_length'].append(valid_response_length)
            else:
                reward_extra_info['wrong_response_length'].append(valid_response_length)

            if isinstance(all_scores, dict):
                reward = all_scores["final_reward"]
                # å­˜å‚¨ä¿¡æ¯
                for key, value in all_scores.items():
                    reward_extra_info[key].append(value)
                if self.num_examine == 1:
                    reward = all_scores["accuracy"] # éªŒè¯æ—¶ä½¿ç”¨
            else:
                if self.num_examine == 1:
                    reward = all_scores if all_scores > 0.6 else 0.0
                else:
                    reward = all_scores

            # è®°å½•åˆ°reward tensor
            reward_tensor[i, valid_response_length - 1] = reward 

            # æ‰“å°ç¤ºä¾‹
            if data_source not in already_print_data_sources:
                already_print_data_sources[data_source] = 0

            if already_print_data_sources[data_source] < self.num_examine:
                already_print_data_sources[data_source] += 1
                print("\n" + "="*80)
                print(f"[ç¤ºä¾‹ {already_print_data_sources[data_source]}]")
                print(f"[prompt] {prompt_str[:200]}...")
                print(f"[response] {response_str}")
                print(f"[ground_truth] ä½ç½® {ground_truth_position}")
                print(f"[num_candidates] {num_candidates}")
                print("[å¾—åˆ†]")
                for key, value in all_scores.items():
                    if isinstance(value, (int, float)):
                        print(f"  {key}: {value:.4f}")
                    else:
                        print(f"  {key}: {value}")
                print("="*80 + "\n")
                    
            # ä¿å­˜è®°å½•
            to_save_prompt = self.tokenizer.decode(valid_prompt_ids, skip_special_tokens=False)
            to_save_response = self.tokenizer.decode(response_ids[:valid_response_length], skip_special_tokens=False)
            if 'responses_with_loss_mask' in data_item.batch:
                to_save_response_with_loss_mask = self.tokenizer.decode(valid_response_ids_with_loss_mask, skip_special_tokens=False)
            
            to_save_records.append({
                'id': data_item.non_tensor_batch['extra_info']['id'] if 'id' in data_item.non_tensor_batch['extra_info'] else None,
                'data_source': data_source,
                "prompt": to_save_prompt,
                "response": to_save_response,
                'response_with_loss_mask': to_save_response_with_loss_mask if 'responses_with_loss_mask' in data_item.batch else None,
                'ground_truth_position': ground_truth_position,
                'score': all_scores,
                'reward': reward,
                'extra_info': data_item.non_tensor_batch.get('extra_info', None),
            })
            
        if save_record:
            # ä¿å­˜è®°å½•åˆ°æ–‡ä»¶
            if self.num_examine == 1:
                temp_file = self.record_dir / f"{self.name}-step-val-{self.step}.json"
            else:
                temp_file = self.record_dir / f"{self.name}-step-{self.step}.json"
            self.step += 1
            temp_file.parent.mkdir(parents=True, exist_ok=True)
            if temp_file.exists() and temp_file.stat().st_size > 0: 
                try:
                    with open(temp_file, "r") as f:
                        existing_records = json.load(f)
                    to_save_records = existing_records + to_save_records
                except (json.JSONDecodeError, ValueError) as e:
                    print(f"âš ï¸ è­¦å‘Š: æ— æ³•ä» {temp_file} åŠ è½½ç°æœ‰è®°å½•: {e}")
            with open(temp_file, "w") as f:
                json.dump(to_save_records, f, indent=4)
            print(f"è®°å½•å·²ä¿å­˜åˆ° {temp_file}")
        
        correct_response_length_mean = np.mean(reward_extra_info['correct_response_length']) if reward_extra_info['correct_response_length'] else 0.0
        wrong_response_length_mean = np.mean(reward_extra_info['wrong_response_length']) if reward_extra_info['wrong_response_length'] else 0.0
        reward_extra_info['correct_response_length'] = [correct_response_length_mean] * len(reward_tensor)
        reward_extra_info['wrong_response_length'] = [wrong_response_length_mean] * len(reward_tensor)

        if return_dict:
            return {
                "reward_tensor": reward_tensor,
                "reward_extra_info": reward_extra_info,
            }
        else:
            return reward_tensor

if __name__ == "__main__":
    print("="*80)
    print("ğŸ§ª çº¯æ–‡æœ¬CoTå¥–åŠ±ç®¡ç†å™¨ - å•å…ƒæµ‹è¯•")
    print("="*80)
    
    # ============= æµ‹è¯• 1: æ ¼å¼å¥–åŠ±æµ‹è¯• =============
    print("\n### æµ‹è¯• 1: æ ¼å¼å¥–åŠ± ###")
    
    test_cases_format = [
        {
            "name": "å®Œç¾æ ¼å¼",
            "response": "<think>è®©æˆ‘åˆ†æ...</think><answer>[2,3,4,1,5]</answer>",
            "num_candidates": 5,
            "expected": {"is_valid_format": 1.0, "is_list_format": 1.0, "format_reward": 1.0}
        },
        {
            "name": "ç¼ºå°‘thinkæ ‡ç­¾",
            "response": "<answer>[2,3,4,1,5]</answer>",
            "num_candidates": 5,
            "expected": {"is_valid_format": 0.0, "format_reward": 0.0}
        },
        {
            "name": "ç´¢å¼•è¶Šç•Œ",
            "response": "<think>æ€è€ƒä¸­...</think><answer>[2,3,4,1,10]</answer>",
            "num_candidates": 5,
            "expected": {"is_valid_format": 1.0, "is_list_format": 1.0}
        },
        {
            "name": "é•¿åº¦ä¸ä¸€è‡´",
            "response": "<think>æ€è€ƒä¸­...</think><answer>[2,3,4]</answer>",
            "num_candidates": 5,
            "expected": {"is_valid_format": 1.0, "is_list_format": 1.0}
        },
        {
            "name": "éåˆ—è¡¨æ ¼å¼",
            "response": "<think>æ€è€ƒä¸­...</think><answer>3</answer>",
            "num_candidates": 5,
            "expected": {"is_valid_format": 1.0, "is_list_format": 0.0, "format_reward": 0.0}
        },
    ]
    
    for i, test in enumerate(test_cases_format, 1):
        result = compute_format_reward(test["response"], test["num_candidates"])
        print(f"\næµ‹è¯•ç”¨ä¾‹ {i}: {test['name']}")
        print(f"  Response: {test['response'][:60]}...")
        print(f"  Results:")
        for key in ['is_valid_format', 'is_list_format', 'format_reward', 'index_penalty', 'length_penalty']:
            value = result.get(key, 0.0)
            print(f"    {key}: {value:.3f}")
        
        # éªŒè¯å…³é”®æŒ‡æ ‡
        for key, expected_val in test["expected"].items():
            actual_val = result[key]
            status = "âœ…" if abs(actual_val - expected_val) < 0.01 else "âŒ"
            print(f"  {status} {key}: expected={expected_val}, actual={actual_val:.3f}")
    
    # ============= æµ‹è¯• 2: æ’åºå¥–åŠ±æµ‹è¯• =============
    print("\n\n### æµ‹è¯• 2: æ’åºå¥–åŠ±ï¼ˆé«˜æ–¯æ ¸ï¼‰ ###")
    
    test_cases_ranking = [
        {
            "name": "ç¬¬1åï¼ˆå®Œç¾ï¼‰",
            "ranking": [2, 3, 4, 1, 5],
            "gt": 2,
            "expected_rank": 1,
            "expected_reward": 1.0,
        },
        {
            "name": "ç¬¬2å",
            "ranking": [3, 2, 4, 1, 5],
            "gt": 2,
            "expected_rank": 2,
            "expected_reward": 0.32,  # exp(-1/(2*0.5^2)) â‰ˆ 0.135
        },
        {
            "name": "ç¬¬3å",
            "ranking": [3, 4, 2, 1, 5],
            "gt": 2,
            "expected_rank": 3,
            "expected_reward": 0.02,  # exp(-4/(2*0.5^2)) â‰ˆ 0.018
        },
        {
            "name": "ä¸åœ¨æ’åºä¸­",
            "ranking": [1, 3, 4, 5],
            "gt": 2,
            "expected_rank": -1,
            "expected_reward": 0.0,
        },
    ]
    
    for i, test in enumerate(test_cases_ranking, 1):
        reward, details = compute_ranking_reward(test["ranking"], test["gt"], sigma=0.5)
        print(f"\næµ‹è¯•ç”¨ä¾‹ {i}: {test['name']}")
        print(f"  Ranking: {test['ranking']}")
        print(f"  Ground Truth: {test['gt']}")
        print(f"  Rank Position: {details['rank_position']}")
        print(f"  Gaussian Reward: {reward:.4f}")
        
        status_rank = "âœ…" if details['rank_position'] == test["expected_rank"] else "âŒ"
        status_reward = "âœ…" if abs(reward - test["expected_reward"]) < 0.1 else "âŒ"
        print(f"  {status_rank} Expected Rank: {test['expected_rank']}")
        print(f"  {status_reward} Expected Reward: {test['expected_reward']:.3f}")
    
    # ============= æµ‹è¯• 3: ç»¼åˆå¥–åŠ±æµ‹è¯• =============
    print("\n\n### æµ‹è¯• 3: ç»¼åˆå¥–åŠ±ï¼ˆæ ¼å¼ + æ’åºï¼‰ ###")
    
    test_cases_combined = [
        {
            "name": "å®Œç¾ç­”æ¡ˆ",
            "response": "<think>åŸºäºæ–‡æœ¬ç›¸ä¼¼æ€§ï¼Œæˆ‘å°†å®ƒä»¬æ’åºä¸º...</think><answer>[2,3,4,1,5]</answer>",
            "gt": 2,
            "num_candidates": 5,
        },
        {
            "name": "æ ¼å¼é”™è¯¯ä½†ç­”æ¡ˆå¯¹",
            "response": "<think>åˆ†æ...</think><answer>2</answer>",
            "gt": 2,
            "num_candidates": 5,
        },
        {
            "name": "æ ¼å¼å¯¹ä½†æ’åºå·®",
            "response": "<think>è®©æˆ‘æƒ³æƒ³...</think><answer>[5,4,3,1,2]</answer>",
            "gt": 2,
            "num_candidates": 5,
        },
        {
            "name": "æ ¼å¼å’Œæ’åºéƒ½å·®",
            "response": "<think>æˆ‘è®¤ä¸º...</think><answer>5</answer>",
            "gt": 2,
            "num_candidates": 5,
        },
    ]
    
    for i, test in enumerate(test_cases_combined, 1):
        result = compute_reward(
            test["response"],
            test["gt"],
            test["num_candidates"],
            sigma=0.5,
            format_weight=0.3,
            ranking_weight=0.7
        )
        print(f"\næµ‹è¯•ç”¨ä¾‹ {i}: {test['name']}")
        print(f"  Response: {test['response'][:60]}...")
        print(f"  Ground Truth: {test['gt']}")
        print(f"  Results:")
        print(f"    Format Reward: {result['format_reward']:.3f}")
        print(f"    Ranking Reward: {result['ranking_reward']:.3f}")
        print(f"    Final Reward: {result['final_reward']:.3f} (0.3*{result['format_reward']:.2f} + 0.7*{result['ranking_reward']:.2f})")
    
    print("\n" + "="*80)
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("="*80)